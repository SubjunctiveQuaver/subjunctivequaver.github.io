<!DOCTYPE html><html lang="en-AU" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.1" /><meta property="og:title" content="Why probability and statistics need measure theory, part 2" /><meta property="og:locale" content="en_AU" /><meta name="description" content="If you haven’t already read part 1, make sure you read it here first! Or else, much of the below will not make sense!" /><meta property="og:description" content="If you haven’t already read part 1, make sure you read it here first! Or else, much of the below will not make sense!" /><link rel="canonical" href="https://subjunctivequaver.github.io//posts/measure-theory-in-probability-2/" /><meta property="og:url" content="https://subjunctivequaver.github.io//posts/measure-theory-in-probability-2/" /><meta property="og:site_name" content="Epic Maths Time" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-06-18T13:14:00+10:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Why probability and statistics need measure theory, part 2" /><meta name="twitter:site" content="@SubjunctiveQuav" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-06-27T21:33:01+10:00","datePublished":"2021-06-18T13:14:00+10:00","description":"If you haven’t already read part 1, make sure you read it here first! Or else, much of the below will not make sense!","headline":"Why probability and statistics need measure theory, part 2","mainEntityOfPage":{"@type":"WebPage","@id":"https://subjunctivequaver.github.io//posts/measure-theory-in-probability-2/"},"url":"https://subjunctivequaver.github.io//posts/measure-theory-in-probability-2/"}</script><title>Why probability and statistics need measure theory, part 2 | Epic Maths Time</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Epic Maths Time"><meta name="application-name" content="Epic Maths Time"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://raw.githubusercontent.com/SubjunctiveQuaver/subjunctivequaver.github.io/main/SubjunctiveQuaver.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Epic Maths Time</a></div><div class="site-subtitle font-italic">Lawrence's blog on epic mathematical tidbits!</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/SubjunctiveQuaver" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/SubjunctiveQuav" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['lawrence.d.chen','outlook.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <a href="https://www.linkedin.com/in/lawrencechen11/" aria-label="linkedin" class="order-7" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="" aria-label="" class="order-8" target="_blank" rel="noopener"> <i class=""></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Why probability and statistics need measure theory, part 2</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Why probability and statistics need measure theory, part 2</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Lawrence Chen </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Jun 18, 2021, 1:14 PM +1000" >Jun 18, 2021<i class="unloaded">2021-06-18T13:14:00+10:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Jun 27, 2021, 9:33 PM +1000" >Jun 27, 2021<i class="unloaded">2021-06-27T21:33:01+10:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3075 words">17 min read</span></div></div><div class="post-content"><p>If you haven’t already read part 1, make sure you read it <a href="https://subjunctivequaver.github.io/posts/measure-theory-in-probability/">here</a> first! Or else, much of the below will not make sense!</p><h2 id="random-variables-neither-random-nor-a-variable">Random variables: neither random, nor a variable</h2><h3 id="what-is-a-random-variable">What is a random variable?</h3><p>Let \((\Omega,\mathcal F,\mathbb P)\) be a probability space.</p><p><strong>Definition 10.</strong> A <strong>random variable</strong> is a <em>measurable</em> function \(X : \Omega \to E\), where \((E,\mathcal E)\) is the <strong>state space</strong>. We usually take \(E\) to be a topological space \((E,\mathcal T)\) (e.g. \(\mathbb R,\mathbb R^n,\mathbb C\) with the usual topologies), so that \((E,\mathcal B)\) is endowed with the <em>Borel sigma algebra</em>.</p><p>Clearly we must define measurable functions.</p><p><strong>Definition 11.</strong> A function \(f : (\Omega,\mathcal F) \to (E,\mathcal E)\) between measurable spaces is <strong>measurable</strong> if, for any measurable subset \(A \in \mathcal E\) of \(E\), its <em>preimage</em></p>\[f^{-1}(A) := \{x \in \Omega : f(x) \in A\}\]<p>is measurable in \(\Omega\), i.e. \(f^{-1}(A) \in \mathcal F\). If \((E,\mathcal T)\) is a topological space and \(\mathcal E = \mathcal B(\mathcal T)\), then \(f\) is <strong>Borel measurable</strong>.</p><p>For now, we will take \(\Omega = E = \mathbb R\), and the Borel sigma algebra on \(\mathbb R\). What are some examples of measurable functions? Well, it turns out that every function you can think of (well, with probability 1) will be measurable! One particularly nice class of measurable functions in this case are the <em>continuous functions</em>:</p><p><strong>Definition 12.</strong> A function \(f : (\Omega,\tau) \to (E,\mathcal T)\) between <em>topological spaces</em> is <strong>continuous</strong> if, for any open subset \(A \in \mathcal T\) of \(E\), its <em>preimage</em> \(f^{-1}(A)\) is open in \(\Omega\), i.e. \(f^{-1}(A) \in \tau\).</p><p>Hopefully you can see the similarity: just replace “measurable” with “open”! Again, this may be different to the usual notion of continuity that you know (nearby inputs map to nearby outputs), but they turn out to be <a href="https://math.stackexchange.com/questions/2762135/equivalence-of-continuity-between-metric-and-topological-spaces">equivalent</a>. Here is a quick proof of the above claim:</p><p><strong>Proposition 13.</strong> Suppose \(f : (\Omega,\tau) \to (E,\mathcal T)\) is a <em>continuous</em> function. Then for a sigma algebra \(\mathcal F\) on \(\Omega\) that <em>contains</em> the Borel sigma algebra \(\mathcal B(\tau)\), the function \(f : (\Omega,\mathcal F) \to (E,\mathcal B(\mathcal T))\) is <em>measurable</em>.</p><p><em>Proof.</em> Let \(A \in \mathcal B(\mathcal T)\) be a Borel set. Recall that this means that there is a countable sequence of union/intersection/complement operations such that \(A\) is constructed from a (countable) family of open sets \((A_i)_{i \in I}\). But note that the preimage of any union/intersection/complement is the union/intersection/complement of the preimages (in general, \(f^{-1}(A \cup B) = f^{-1}(A) \cup f^{-1}(B)\), \(f^{-1}(A \cap B) = f^{-1}(A) \cap f^{-1}(B)\), and \(f^{-1}(A \setminus B) = f^{-1}(A) \setminus f^{-1}(B)\)), so it follows that \(f^{-1}(A)\) is constructed from the sets \((f^{-1}(A_i))_{i \in I}\) using the exact same sequence of operations. But \(f^{-1}(A_i)\) is open for any \(i\) by continuity of \(f\) (by definition), thus measurable (since \(\mathcal F\) contains \(\mathcal B(\tau)\), which contains all open sets in \(\Omega\)). So \(f^{-1}(A)\) is constructed from the measurable sets \((f^{-1}(A_i))_{i \in I}\) using a countable sequence of union/intersection/complement operations, so \(f^{-1}(A) \in \mathcal F\) (sigma algebras are closed under these operations). \(\square\)</p><p>This immediately gives many, many measurable functions! Assuming the sample space is \(\mathbb R\), polynomial functions, rational functions, exponentials, trigonometric functions, logarithmic functions etc. are all measurable, and so are their sums, products, quotients (where defined), and compositions (which are all continuous)! So are the minimum/maximum of two continuous/measurable functions (in fact, supremums and infimums also work). So is the wild <a href="https://en.wikipedia.org/wiki/Weierstrass_function">Weierstrass function</a>, which is differentiable nowhere, but continuous everywhere, thus measurable!</p><p><img data-proofer-ignore data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/WeierstrassFunction.svg/2880px-WeierstrassFunction.svg.png" alt="The Weierstrass function, which is measurable." /></p><p>But it turns out that many discontinuous functions are also measurable. Let’s look at one example: the <em>signum</em> function</p>\[\operatorname{sgn} : \mathbb R \to \mathbb R, \quad x \mapsto \begin{cases} 1, &amp; x &gt; 0 \\ 0, &amp; x = 0 \\ -1, &amp; x &lt; 0 \end{cases}\]<p>(a friend-favourite, for some reason…). To show this, we need to show that for any Borel set \(A \subseteq \mathbb R\), its preimage \(\operatorname{sgn}^{-1}(A) = \{x \in \mathbb R : \operatorname{sgn}(x) \in A\}\) is again a Borel set in \(\mathbb R\). One approach considers 8 cases; we do only one. Suppose that \(0,1 \in A\) but \(-1 \not\in A\). Then</p>\[\operatorname{sgn}^{-1}(A) = \{x \in \mathbb R : \operatorname{sgn}(x) \in A\} = \{x \in \mathbb R : \operatorname{sgn}(x) \in \{0,1\}\}\]<p>since \(\operatorname{sgn}\) only takes on values in \(\{0,\pm 1\}\). Therefore, \(\operatorname{sgn}^{-1}(A) = [0,\infty)\); this is a Borel set since its complement is the open set \((-\infty,0)\) (and open sets are always Borel sets).</p><p><strong>Challenge question 5.</strong> Complete the above proof that the signum function is measurable by identifying the remaining 7 cases, and checking that \(\operatorname{sgn}^{-1}(A)\) is a Borel set in each case.</p><p>Going back to our example 9 with the uniform distribution on \([0,1]\), it now follows that the inclusion map \(X : \Omega \hookrightarrow \mathbb R, x \mapsto x\), is a <em>random variable</em>, since it is continuous (thus measurable). This is how we typically think of a random variable with a uniform distribution!</p><p>Briefly, let’s consider random variables from finite probability spaces. Since the natural sigma algebra is the power set (the <em>discrete topology</em>), it follows that <em>every</em> function is measurable. So <em>any</em> function \(X : \Omega \to E\) is a valid random variable. Let’s now look at random variables in a bit more depth.</p><h3 id="fun-with-random-variables">Fun with random variables</h3><p>Let’s first introduce some notation. Recall that a random variable \(X : \Omega \to E\) is a <em>measurable function</em> from a probability space \((\Omega,\mathcal F,\mathbb P)\) to a measurable space \((E,\mathcal E)\). It’s not a <em>variable</em>! And it’s not even random… the “randomness” comes from the fact that a probability measure assigns “chances” to different events. Let’s now combine the notion of <em>random variable</em>, with the notion of <em>probability space</em>. This is how you’ve learnt random variables in high school/early university!</p><p>For the following, we take \(E = \mathbb R\). Let \(A \subseteq \mathbb R\) be a Borel set. Then since \(X\) is measurable, \(X^{-1}(A)\) is a valid event. Thus we may take its probability, and we write it in the following ways:</p>\[\mathbb P(X^{-1}(A)) = \mathbb P(\{\omega \in \Omega : X(\omega) \in A\}) = \mathbb P(X \in A).\]<p>Of these, the last is probably the most familiar. But they all mean the same thing! In fact, what we <em>mean</em> when we write \(X \in A\), is precisely the event \(X^{-1}(A)\)!</p><p><strong>Example 8 (continued from part 1).</strong> Recall this example, in which we had an infinite sequence of coin tosses and a sample space \(\Omega = \{0,1\}^\infty\). For \(n \geq 1\), consider the following random variable \(X_n : \Omega \to \mathbb R,\)</p>\[\omega = (\omega_1,\omega_2,...) \mapsto \omega_n,\]<p>which is simply projection onto the \(n\)th coordinate. For example, if \(n = 2\) and we consider the outcome \(\omega = (1,0,1,0,...)\), we get \(X_2(\omega) = 0\). We can see that this essentially measures the outcome of the \(n\)th toss: if it was a tail, then \(X_n(\omega) = 0\); if a head, then \(X_n(\omega) = 1\). Assuming that each toss independently has probability \(p \in (0,1)\) of appearing as a head, we observe the following: for any \(n\),</p>\[\mathbb P(X_n = 1) = \mathbb P(X_n^{-1}(\{1\})) = \mathbb P(\{\omega \in \Omega : X_n(\omega) = 1\}) = p,\]<p>by our assertion. Similarly, \(\mathbb P(X_n = 0) = \mathbb P(\{\omega \in \Omega : X_n(\omega) = 0\}) = 1 - p\), since</p>\[X_n^{-1}(0) \cup X_n^{-1}(1) = X_n^{-1}(\{0,1\}) = \Omega\]<p>and</p>\[X_n^{-1}(0) \cap X_n^{-1}(1) = X_n^{-1}(\{0\} \cap \{1\}) = X_n^{-1}(\varnothing) = \varnothing,\]<p>i.e. the two events \(\{X_n = 0\}\) and \(\{X_n = 1\}\) are disjoint (as expected), and their union is the sample space (this is called a <strong>partition</strong>), so the sum of the above two probabilities should be 1. (Note that here we use general properties of preimages of functions (\(f^{-1}(A \cup B) = f^{-1}(A) \cup f^{-1}(B)\), \(f^{-1}(A \cap B) = f^{-1}(A) \cap f^{-1}(B)\), and \(f^{-1}(A \setminus B) = f^{-1}(A) \setminus f^{-1}(B)\)), and when taking the preimage of a point, we often drop the curly brackets. It does <em>not</em> mean the inverse function; \(X_n\) is certainly not invertible!)</p><p><strong>Example 9 (continued from part 1).</strong> Let’s again consider the uniform distribution on \([0,1]\), and the random variable \(X : \Omega \hookrightarrow \mathbb R, x \mapsto x\) defined above. Then, for example, for \((a,b) \subseteq (0,1)\),</p>\[\mathbb P(a &lt; X &lt; b) = \mathbb P(X^{-1}((a,b))) = \mathbb P((a,b)) = b - a.\]<p>Now, recall that the composition of measurable functions is measurable, so if \(f\) is for instance continuous, then \(f(X) = f \circ X\) is also a random variable! Let’s take \(f\) to be the squaring function \(x \mapsto x^2\). Let’s investigate the random variable \(f(X) = X^2 : [0,1] \to \mathbb R\) where \(X^2(\omega) = \omega^2\):</p><p><strong>Challenge question 5 (related to example 9).</strong></p><ol><li>Find \((X^2)^{-1}(I) \subseteq [0,1]\) for any open interval \(I = (a,b) \subseteq [0,1]\). Is this a valid event (a Borel set in \([0,1]\))?<li>Thus compute the probability \(\mathbb P(a &lt; X^2 &lt; b)\).<li>For \(x \in (0,1)\), find \((X^2)^{-1}((-\infty,x])\) and thus find \(G(x) = \mathbb P(X^2 \leq x)\). What is \(G(b) - G(a)\)?<li>Can you find a function \(g : (0,1) \to \mathbb R\) such that \(\mathbb P(a &lt; X^2 &lt; b) = \int_a^b g(x)\,dx\)? (<em>Hint:</em> consider the function \(G : (0,1) \to [0,1]\), and apply the <em>fundamental theorem of calculus</em>, which implies \(\int_a^b G'(x)\,dx = G(b) - G(a)\).) This is (almost) the <em>probability density function</em> (or <em>pdf</em>) of \(X^2\).</ol><p>Post your solutions in the unofficial <a href="https://discord.gg/hx63ZwSXBg">Maths @ Monash Discord</a>!</p><h2 id="distributions-a-change-in-perspective">Distributions: a change in perspective</h2><h3 id="probability-distributions-and-pushforwards">Probability distributions and pushforwards</h3><p>A subtle but important change of perspective is to notice that for a probability measure \(\mathbb P\) on the sample space, and a random variable \(X : \Omega \to E\), we can actually define a <em>new</em> probability measure on the state space \(E\), that arises naturally from \(X\)! This is the <em>pushforward measure</em> \(\mathbb P_X = X_*\mathbb P = \mathbb P \circ X^{-1} : \mathcal E \to [0,1]\), defined by</p>\[\mathbb P_X(A) := \mathbb P(X^{-1}(A)) = \mathbb P(X \in A).\]<p>Here, \(A \in \mathcal E\) is a (Borel) measurable set. Then the triple \((E,\mathcal E,\mathbb P_X)\) is a <em>probability space</em>, induced by the random variable \(X\), so we can directly talk about probabilities using only the state space! And this is crucial in defining <em>probability distributions</em>, as we can then largely ignore the sample space!</p><p>This is something that we like to do a lot in maths: which is to start off with a structure on a set, and use a mapping to <em>transfer</em> that structure over to a different set. This is one such example; other examples of similar structure-preserving maps are continuous maps (preserving open sets), homeomorphisms (preserving topologies), isometries (preserving distances), linear transformations (preserving vector space structure), homomorphisms/isomorphisms (preserving algebraic structure), and pullbacks of bilinear forms under a linear map.</p><p>Now, let’s consider \((-\infty,x]\) which is clearly a Borel set. We define the cumulative distribution function using the probability that \(X\) takes on a value in this set:</p><p><strong>Definition 14.</strong> For a random variable \(X : \Omega \to \mathbb R\), its <strong>(cumulative) distribution function (cdf)</strong> is the measurable function \(F_X : \mathbb R \to [0,1]\), defined by</p>\[F_X(x) = \mathbb P(X \leq x) = \mathbb P(X^{-1}((-\infty,x])) = \mathbb P_X((-\infty,x]).\]<p>That is, the cdf of \(X\) gives us the probability measure of the event \((-\infty,x]\), where we push the probability measure from the sample space over to the state space! Similarly, if we have a probability space \((\Omega,\mathcal F,\mathbb P)\) with sample space \(\Omega \subseteq \mathbb R\), we can define the <strong>distribution function</strong> of the probability measure \(\mathbb P\) as \(F : \mathbb R \to [0,1]， x \mapsto \mathbb P((-\infty,x])\).</p><p>As a quick note, the cdf of a random variable \(X\) fully determines its properties, as we may recover the event \(X^{-1}(A) = \{X \in A\}\) (thus \(\mathbb P(X \in A)\)) for any Borel set \(A\) from countable unions/intersections/complements of events \(X^{-1}((-\infty,x]) = \{X \leq x\}\) with \(x \in \mathbb R\).</p><p>It is possible to see that any cdf must be <em>increasing</em> (i.e. if \(a &lt; b\), then \(F(a) \leq F(b)\)):</p><p><em>Proof</em> For \(a &lt; b\), we have</p>\[F(b) = \mathbb P((-\infty,b]) = \mathbb P((-\infty,a] \cup (a,b]) = \mathbb P((-\infty,a]) + \underbrace{\mathbb P((a,b])}_{\geq 0} \geq F(a). \square\]<p>Additionally, cdfs have <a href="https://math.stackexchange.com/questions/147612/discontinuity-points-of-a-distribution-function"><em>countably many discontinuities</em></a>, and must be <em>right-continuous</em>, meaning that not <em>every</em> increasing measurable function with codomain \([0,1]\) is the cdf of some random variable.</p><p>This change of perspective is useful, as it allows us to define when two random variables have the <em>same probability distribution</em>, even when they aren’t defined on the <em>same probability space</em>! We say that two random variables \(X : (\Omega_1,\mathcal F_1,\mathbb P_1) \to (\mathbb R,\mathcal B)\) and \(Y : (\Omega_2,\mathcal F_2,\mathbb P_2) \to (\mathbb R,\mathcal B)\) are <strong>equal in distribution</strong> if their cdfs are equal as functions: \(F_X = F_Y\); this means that \(\mathbb P_1(X \leq x) = \mathbb P_2(Y \leq x)\) for all \(x \in \mathbb R\) (note the different probability measures), or equivalently,</p>\[\mathbb P_X((-\infty,x]) = \mathbb P_Y((-\infty,x]),\]<p>which turns out to imply that \(\mathbb P_X = \mathbb P_Y\) (the <em>pushforward measures</em>) as probability measures on the state space \((\mathbb R,\mathcal B)\) (again via the <a href="https://handwiki.org/wiki/Hahn%E2%80%93Kolmogorov_theorem">Hahn-Kolmogorov theorem</a>). And hopefully this explains why the pushforward measure is so important: \(X\) and \(Y\) have the same probability distribution if and only if they induce identical probability spaces on the event space (via the pushforward measure), even if they are defined on totally different sample spaces.</p><p>Furthermore, this change in perspective allows us to “discard” the sample space, and instead consider a probability measure on the state space \(\mathbb R\), in the sense that if \((\Omega,\mathcal F,\mathbb P)\) is a probability space and \(X : \Omega \to \mathbb R\) is a random variable with pushforward measure \(\mathbb P_X\), we ignore this and work only with the probability space \((\mathbb R,\mathcal B,\mathbb P_X)\), and we consider the properties of this probability space (expectation, density, etc.). This is the usual approach to learning random variables in early studies – do you ever remember thinking about the sample space when working with random variables? Most likely not.</p><h3 id="examples-of-distributions">Examples of distributions</h3><p><strong>Example 15.</strong> Lets see an example of this, coming from our previous examples. Consider the first probability space to be the example with rolling two fair dice (independently) with \(\Omega_1 = \{1,...,6\} \times \{1,...,6\}\), and the second probability space be the uniform distribution on \(\Omega_2 = [0,1]\). Clearly these random variables are vastly different. Define the random variable \(X : \Omega_1 \to \mathbb R\) as giving \(1\) if the first die resulted in an equal or higher roll than the second, or \(0\) otherwise:</p>\[X(\omega) = X((\omega_1,\omega_2)) = \begin{cases} 1, &amp; \omega_1 \geq \omega_2 \\ 0, &amp; \omega_1 &lt; \omega_2 \end{cases}\]<p>Let \(Y : \Omega_2 \to \mathbb R\) be given by the following transformation:</p>\[Y(\omega) = \begin{cases} 1, &amp; \omega \leq 21/36 \\ 0, &amp; \omega &gt; 21/36 \end{cases}\]<p>Let’s compute the cdfs of \(X\) and \(Y\). For \(X\), for any \(x \in (-\infty,0)\), we have \(X^{-1}((-\infty,x]) = \varnothing\), so \(F_X(x) = \mathbb P_X((-\infty,x]) = \mathbb P_1(\varnothing) = 0\). For \(x \in [0,1)\), \(X^{-1}((-\infty,x]) = X^{-1}(0) = \{(\omega_1,\omega_2) \in \Omega_1 : \omega_1 \geq \omega_2\}\), so</p>\[F_X(x) = \mathbb P_X((-\infty,x]) = \mathbb P_1(\{(\omega_1,\omega_2) \in \Omega_1 : \omega_1 \geq \omega_2\}) = \frac{21}{36}\]<p>by a simple combinatorial argument, and the fact that the 21 outcomes in that event are equiprobable. Now for \(x \in [1,\infty)\), \(X^{-1}((-\infty,x]) = X^{-1}(\{0,1\}) = \Omega_1\), so \(F_X(x) = \mathbb P_X((-\infty,x]) = \mathbb P_1(\Omega_1) = 1\). In summary:</p>\[F_X(x) = \begin{cases} 0, &amp; x &lt; 0 \\ 21/36, &amp; 0 \leq x &lt; 1 \\ 1, &amp; x \geq 1 \end{cases}\]<p><strong>Challenge question 6 (related to example 15).</strong> Compute the cdf of \(Y\), and show that \(X\) and \(Y\) are equal in distribution. (This then implies \(\mathbb P_X = \mathbb P_Y\) and \(\mathbb P_1(X \in A) = \mathbb P_2(Y \in A)\) for any Borel set \(A\), and we may as well forget about the original sample spaces and consider the abstract properties of their common induced probability space on \(\mathbb R\)!) If you need a reminder of how \((\Omega_2,\mathcal F_2,\mathbb P_2)\) works, the example is Example 9 in <a href="https://subjunctivequaver.github.io/posts/measure-theory-in-probability/">here</a>.</p><p>At this point, we are finally ready to <em>define</em> discrete and continuous random variables, or more fundamentally, probability distributions.</p><p><strong>Definition 16.</strong></p><ol><li>A <strong>continuous random variable</strong> \(X : \Omega \to \mathbb R\) is such that \(\mathbb P(X = x) = \mathbb P_X(\{x\}) = 0\) for all \(x \in \mathbb R\).<li>An <strong>absolutely continuous random variable</strong> \(X : \Omega \to \mathbb R\) is such that there exists a function \(f : \mathbb R \to [0,\infty]\) such that \(\mathbb P(X \in A) = \int_A f(x)\,dx\) for <em>any</em> Borel set \(A \subseteq \mathbb R\). This function \(f\) is the <strong>probability density function (density/pdf)</strong> of \(X\). (Absolutely continuous random variables are continuous.)<li>A <strong>discrete random variable</strong> \(X : \Omega \to \mathbb R\) is such that there exists a <em>countable</em> Borel set \(A\) with \(\mathbb P(X \in A) = \mathbb P_X(A) = 1\). (If \(\Omega\) is discrete, any random variable will be discrete.)</ol><p>Note how densities are defined. We start with a probability space, with a probability measure \(\mathbb P\) defined on it, and a random variable \(X\) mapping the sample space into the reals. A density is then a function that satisfies the particular property that</p>\[\mathbb P(X \in A) = \mathbb P_X(A) = \int_A f(x)\,dx\]<p>for <em>any</em> Borel set \(A \subseteq \mathbb R\), which may feel backwards. Don’t we usually start with the density function? We can, and again this approach uses the idea that we can forget about the original sample space, and study the probability space via the pushforward measure. So when we gave the normal density</p>\[f_\theta : \mathbb{R} \to \mathbb{R}, \quad x \mapsto \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)\]<p>at the start of part 1, we can either think of a probability space and a random variable which happens to have this as a density, or we can think of this density as characterising the pushforward measure of <em>some</em> random variable whose sample space is unimportant. And it is this second perspective that we tend to prefer, as the analysis (probability calculations, expectation/moments, statistics, etc.) then directly applies to <em>any</em> scenario in which the pushforward measure has the same density, even if the underlying experiment is different (as we saw in Example 15).</p><p>Next up, to tackle the problem of unifying the concepts of densities and expectations of different types of distributions, we will consider a new form of integral: the Lebesgue integral. We leave this to the next part of this series!</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/epic-maths-time/'>Epic Maths Time</a>, <a href='/categories/new-perspectives/'>New Perspectives</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/probability/" class="post-tag no-text-decoration" >probability</a> <a href="/tags/measure-theory/" class="post-tag no-text-decoration" >measure-theory</a> <a href="/tags/statistics/" class="post-tag no-text-decoration" >statistics</a> <a href="/tags/topology/" class="post-tag no-text-decoration" >topology</a> <a href="/tags/uni-maths/" class="post-tag no-text-decoration" >uni-maths</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Why probability and statistics need measure theory, part 2 - Epic Maths Time&url=https://subjunctivequaver.github.io//posts/measure-theory-in-probability-2/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Why probability and statistics need measure theory, part 2 - Epic Maths Time&u=https://subjunctivequaver.github.io//posts/measure-theory-in-probability-2/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Why probability and statistics need measure theory, part 2 - Epic Maths Time&url=https://subjunctivequaver.github.io//posts/measure-theory-in-probability-2/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/honours-thesis/">My honours thesis — minimum bases for permutation groups</a><li><a href="/posts/generatingfunctionological-proof-geometric-arithmetic-sequences/">A generatingfunctionological proof of the geometric and arithmetic sequence formulas</a><li><a href="/posts/generatingfunctionological-proof-binomial-theorem/">A generatingfunctionological proof of the binomial theorem</a><li><a href="/posts/integrating-rational-functions/">Integrating rational functions, partial fractions, and a taste of algebra, part 1</a><li><a href="/posts/measure-theory-in-probability/">Why probability and statistics need measure theory, part 1</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/uni-maths/">uni-maths</a> <a class="post-tag" href="/tags/algebra/">algebra</a> <a class="post-tag" href="/tags/calculus/">calculus</a> <a class="post-tag" href="/tags/combinatorics/">combinatorics</a> <a class="post-tag" href="/tags/measure-theory/">measure-theory</a> <a class="post-tag" href="/tags/polynomials/">polynomials</a> <a class="post-tag" href="/tags/probability/">probability</a> <a class="post-tag" href="/tags/rings/">rings</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/topology/">topology</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/measure-theory-in-probability/"><div class="card-body"> <span class="timeago small" >Jun 16, 2021<i class="unloaded">2021-06-16T21:57:00+10:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Why probability and statistics need measure theory, part 1</h3><div class="text-muted small"><p> Introduction to the problem You may have encountered continuous probability distributions such as the normal distribution. It’s often used to model things in the real world, and has nice statistic...</p></div></div></a></div><div class="card"> <a href="/posts/generatingfunctionological-proof-geometric-arithmetic-sequences/"><div class="card-body"> <span class="timeago small" >Apr 16, 2022<i class="unloaded">2022-04-16T11:28:00+10:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>A generatingfunctionological proof of the geometric and arithmetic sequence formulas</h3><div class="text-muted small"><p> Recall from high school that a geometric sequence is a sequence \((a_n)_{n \geq 0}\) that satisfies the recurrence relation \(a_{n + 1} = r a_n\) for some fixed \(r \in \mathbb{R}\), and an arithme...</p></div></div></a></div><div class="card"> <a href="/posts/generatingfunctionological-proof-binomial-theorem/"><div class="card-body"> <span class="timeago small" >Apr 16, 2022<i class="unloaded">2022-04-16T13:00:00+10:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>A generatingfunctionological proof of the binomial theorem</h3><div class="text-muted small"><p> Make sure you have read the last post on generating functions first, else proceed at your own risk! Recall that for \(k,n \in \mathbb{N}\), the binomial coefficient is defined by \[\binom{k}{n} =...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/measure-theory-in-probability/" class="btn btn-outline-primary" prompt="Older"><p>Why probability and statistics need measure theory, part 1</p></a> <a href="/posts/desmos-3d-plotter/" class="btn btn-outline-primary" prompt="Newer"><p>An interactive visualisation of immersed surfaces on Desmos</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/SubjunctiveQuav">Lawrence Chen</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/uni-maths/">uni maths</a> <a class="post-tag" href="/tags/algebra/">algebra</a> <a class="post-tag" href="/tags/calculus/">calculus</a> <a class="post-tag" href="/tags/combinatorics/">combinatorics</a> <a class="post-tag" href="/tags/measure-theory/">measure theory</a> <a class="post-tag" href="/tags/polynomials/">polynomials</a> <a class="post-tag" href="/tags/probability/">probability</a> <a class="post-tag" href="/tags/rings/">rings</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/topology/">topology</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
